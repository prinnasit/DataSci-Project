{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p93Ki4vDIjAO",
    "outputId": "d4917608-6bd7-4977-a3cc-1319343f0ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\python312\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scholarly in c:\\python312\\lib\\site-packages (1.7.11)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\python312\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: arrow in c:\\python312\\lib\\site-packages (from scholarly) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python312\\lib\\site-packages (from scholarly) (4.12.3)\n",
      "Requirement already satisfied: bibtexparser in c:\\python312\\lib\\site-packages (from scholarly) (1.4.2)\n",
      "Requirement already satisfied: deprecated in c:\\python312\\lib\\site-packages (from scholarly) (1.2.15)\n",
      "Requirement already satisfied: fake-useragent in c:\\python312\\lib\\site-packages (from scholarly) (1.5.1)\n",
      "Requirement already satisfied: free-proxy in c:\\python312\\lib\\site-packages (from scholarly) (1.1.3)\n",
      "Requirement already satisfied: httpx in c:\\python312\\lib\\site-packages (from scholarly) (0.28.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\python312\\lib\\site-packages (from scholarly) (1.0.1)\n",
      "Requirement already satisfied: selenium in c:\\python312\\lib\\site-packages (from scholarly) (4.27.1)\n",
      "Requirement already satisfied: sphinx-rtd-theme in c:\\python312\\lib\\site-packages (from scholarly) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\python312\\lib\\site-packages (from scholarly) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: sgmllib3k in c:\\python312\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\python312\\lib\\site-packages (from arrow->scholarly) (2.9.0.post0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\python312\\lib\\site-packages (from arrow->scholarly) (2.9.0.20241003)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python312\\lib\\site-packages (from beautifulsoup4->scholarly) (2.6)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in c:\\python312\\lib\\site-packages (from bibtexparser->scholarly) (3.2.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\python312\\lib\\site-packages (from deprecated->scholarly) (1.17.0)\n",
      "Requirement already satisfied: lxml in c:\\python312\\lib\\site-packages (from free-proxy->scholarly) (5.3.0)\n",
      "Requirement already satisfied: anyio in c:\\python312\\lib\\site-packages (from httpx->scholarly) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python312\\lib\\site-packages (from httpx->scholarly) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python312\\lib\\site-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\python312\\lib\\site-packages (from requests[socks]->scholarly) (1.7.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\python312\\lib\\site-packages (from selenium->scholarly) (0.27.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\python312\\lib\\site-packages (from selenium->scholarly) (0.11.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\python312\\lib\\site-packages (from selenium->scholarly) (1.8.0)\n",
      "Requirement already satisfied: sphinx<9,>=6 in c:\\python312\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (8.1.3)\n",
      "Requirement already satisfied: docutils<0.22,>0.18 in c:\\python312\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in c:\\python312\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: Jinja2>=3.1 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.17 in c:\\users\\prinnasit\\appdata\\roaming\\python\\python312\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.18.0)\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.13 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.16.0)\n",
      "Requirement already satisfied: alabaster>=0.7.14 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
      "Requirement already satisfied: imagesize>=1.3 in c:\\python312\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\users\\prinnasit\\appdata\\roaming\\python\\python312\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (24.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\prinnasit\\appdata\\roaming\\python\\python312\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (0.4.6)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\python312\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\python312\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\python312\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\python312\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\python312\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium->scholarly) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->scholarly) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv scholarly requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "zfmmraBMMHnj"
   },
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mRJZr7KHsiR"
   },
   "source": [
    "arXiv **Don't have Cited** cc ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yspuQXbS_ZJr",
    "outputId": "2c433de3-e11a-495a-eee6-e9f579336bc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prinnasit\\AppData\\Local\\Temp\\ipykernel_10248\\2189810817.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Mathematical Logic in Computer Science\n",
      "Authors: ['Assaf Kfoury']\n",
      "Summary: The article retraces major events and milestones in the mutual influences\n",
      "between mathematical logic and computer science since the 1950s.\n",
      "Published: 2018-02-07 22:21:43+00:00\n",
      "URL: http://arxiv.org/abs/1802.03292v1\n",
      "==================================================\n",
      "Title: Ten Research Challenge Areas in Data Science\n",
      "Authors: ['Jeannette M. Wing']\n",
      "Summary: Although data science builds on knowledge from computer science, mathematics,\n",
      "statistics, and other disciplines, data science is a unique field with many\n",
      "mysteries to unlock: challenging scientific questions and pressing questions of\n",
      "societal importance. This article starts with meta-questions about data science\n",
      "as a discipline and then elaborates on ten ideas for the basis of a research\n",
      "agenda for data science.\n",
      "Published: 2020-01-27 21:39:57+00:00\n",
      "URL: http://arxiv.org/abs/2002.05658v1\n",
      "==================================================\n",
      "Title: Defining Data Science\n",
      "Authors: ['Yangyong Zhu', 'Yun Xiong']\n",
      "Summary: Data science is gaining more and more and widespread attention, but no\n",
      "consensus viewpoint on what data science is has emerged. As a new science, its\n",
      "objects of study and scientific issues should not be covered by established\n",
      "sciences. Data in cyberspace have formed what we call datanature. In the\n",
      "present paper, data science is defined as the science of exploring datanature.\n",
      "Published: 2015-01-21 02:41:55+00:00\n",
      "URL: http://arxiv.org/abs/1501.05039v1\n",
      "==================================================\n",
      "Title: Why The Trans Programmer?\n",
      "Authors: ['Skye Kychenthal']\n",
      "Summary: Through online anecdotal evidence and online communities, there is an\n",
      "in-group idea of trans people (specifically trans-feminine individuals)\n",
      "disproportionately entering computer science education & fields. Existing data\n",
      "suggests this is a plausible trend, yet no research has been done into exactly\n",
      "why. As computer science education (traditional schooling or self-taught\n",
      "methods) is integral to working in computer science fields, a simple research\n",
      "survey was conducted to gather data on 138 trans people's experiences with\n",
      "computer science & computer science education. This article's purpose is to\n",
      "shed insight on the motivations for trans individuals choosing computer science\n",
      "paths, while acting as a basis and call to action for further research.\n",
      "Published: 2022-05-03 15:06:23+00:00\n",
      "URL: http://arxiv.org/abs/2205.01553v1\n",
      "==================================================\n",
      "Title: Proceedings 11th Doctoral Workshop on Mathematical and Engineering Methods in Computer Science\n",
      "Authors: ['Jan Bouda', 'Lukáš Holík', 'Jan Kofroň', 'Jan Strejček', 'Adam Rambousek']\n",
      "Summary: MEMICS provides a forum for doctoral students interested in applications of\n",
      "mathematical and engineering methods in computer science. Besides a rich\n",
      "technical programme (including invited talks, regular papers, and\n",
      "presentations), MEMICS also offers friendly social activities and exciting\n",
      "opportunities for meeting like-minded people. MEMICS submissions traditionally\n",
      "cover all areas of computer science (such as parallel and distributed\n",
      "computing, computer networks, modern hardware and its design, non-traditional\n",
      "computing architectures, information systems and databases, multimedia and\n",
      "graphics, verification and testing, computer security, as well as all related\n",
      "areas of theoretical computer science).\n",
      "Published: 2016-12-13 05:47:19+00:00\n",
      "URL: http://arxiv.org/abs/1612.04037v1\n",
      "==================================================\n",
      "Title: FAIR and Open Computer Science Research Software\n",
      "Authors: ['Wilhelm Hasselbring', 'Leslie Carr', 'Simon Hettrick', 'Heather Packer', 'Thanassis Tiropanis']\n",
      "Summary: In computational science and in computer science, research software is a\n",
      "central asset for research. Computational science is the application of\n",
      "computer science and software engineering principles to solving scientific\n",
      "problems, whereas computer science is the study of computer hardware and\n",
      "software design.\n",
      "  The Open Science agenda holds that science advances faster when we can build\n",
      "on existing results. Therefore, research software has to be reusable for\n",
      "advancing science. Thus, we need proper research software engineering for\n",
      "obtaining reusable and sustainable research software. This way, software\n",
      "engineering methods may improve research in other disciplines. However,\n",
      "research in software engineering and computer science itself will also benefit\n",
      "from reuse when research software is involved.\n",
      "  For good scientific practice, the resulting research software should be open\n",
      "and adhere to the FAIR principles (findable, accessible, interoperable and\n",
      "repeatable) to allow repeatability, reproducibility, and reuse. Compared to\n",
      "research data, research software should be both archived for reproducibility\n",
      "and actively maintained for reusability. The FAIR data principles do not\n",
      "require openness, but research software should be open source software.\n",
      "Established open source software licenses provide sufficient licensing options,\n",
      "such that it should be the rare exception to keep research software closed.\n",
      "  We review and analyze the current state in this area in order to give\n",
      "recommendations for making computer science research software FAIR and open. We\n",
      "observe that research software publishing practices in computer science and in\n",
      "computational science show significant differences.\n",
      "Published: 2019-08-16 14:26:08+00:00\n",
      "URL: http://arxiv.org/abs/1908.05986v1\n",
      "==================================================\n",
      "Title: A Polynomial Translation of pi-calculus FCPs to Safe Petri Nets\n",
      "Authors: ['Victor Khomenko', 'Roland Meyer', 'Reiner Hüchting']\n",
      "Summary: We develop a polynomial translation from finite control pi-calculus processes\n",
      "to safe low-level Petri nets. To our knowledge, this is the first such\n",
      "translation. It is natural in that there is a close correspondence between the\n",
      "control flows, enjoys a bisimulation result, and is suitable for practical\n",
      "model checking.\n",
      "Published: 2013-09-03 15:08:39+00:00\n",
      "URL: http://arxiv.org/abs/1309.0717v2\n",
      "==================================================\n",
      "Title: Computer Science\n",
      "Authors: ['Mahyuddin K. M. Nasution', 'Rahmat Hidayat', 'Rahmad Syah']\n",
      "Summary: Possible for science itself, conceptually, to have and will understand\n",
      "differently, let alone science also seen as technology, such as computer\n",
      "science. After all, science and technology are viewpoints diverse by either\n",
      "individual, community, or social. Generally, it depends on socioeconomic\n",
      "capabilities. So it is with computer science has become a phenomenon and\n",
      "fashionable, where based on the stream of documents, various issues arise in\n",
      "either its theory or implementation, adapting different communities, or\n",
      "designing curriculum holds in the education system.\n",
      "Published: 2022-07-16 10:54:57+00:00\n",
      "URL: http://arxiv.org/abs/2207.07901v1\n",
      "==================================================\n",
      "Title: Decidability of higher-order matching\n",
      "Authors: ['Colin Stirling']\n",
      "Summary: We show that the higher-order matching problem is decidable using a\n",
      "game-theoretic argument.\n",
      "Published: 2009-07-22 09:17:30+00:00\n",
      "URL: http://arxiv.org/abs/0907.3804v3\n",
      "==================================================\n",
      "Title: Introduction: Cognitive Issues in Natural Language Processing\n",
      "Authors: ['Thierry Poibeau', 'Shravan Vasishth']\n",
      "Summary: This special issue is dedicated to get a better picture of the relationships\n",
      "between computational linguistics and cognitive science. It specifically raises\n",
      "two questions: \"what is the potential contribution of computational language\n",
      "modeling to cognitive science?\" and conversely: \"what is the influence of\n",
      "cognitive science in contemporary computational linguistics?\"\n",
      "Published: 2016-10-24 11:30:22+00:00\n",
      "URL: http://arxiv.org/abs/1610.07365v1\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Computer Science\"\n",
    "search = arxiv.Search(\n",
    "    query=query,\n",
    "    max_results=10,  # Number of results to fetch\n",
    "    sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n",
    "\n",
    "\n",
    "for result in search.results():\n",
    "    print(f\"Title: {result.title}\")\n",
    "    print(f\"Authors: {[author.name for author in result.authors]}\")\n",
    "    print(f\"Summary: {result.summary}\")\n",
    "    print(f\"Published: {result.published}\")\n",
    "    print(f\"URL: {result.entry_id}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAQrCq2NxGrc",
    "outputId": "ef17cd35-c191-4261-d578-13eca5861f19"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Fetch data using a query\u001b[39;00m\n\u001b[0;32m     94\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPUBYEAR > 2019 AND SUBJAREA(COMP) AND TITLE(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputer\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 95\u001b[0m articles \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_scopus_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Save results to CSV\u001b[39;00m\n\u001b[0;32m     98\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscopus_data_with_abstracts2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m, in \u001b[0;36mfetch_scopus_data\u001b[1;34m(query, count, max_results)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n\u001b[0;32m     32\u001b[0m     scopus_id \u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdc:identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCOPUS_ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprism:url\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: scopus_id,\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meid\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdc:title\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreator\u001b[39m\u001b[38;5;124m\"\u001b[39m: parse_authors(entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdc:creator\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublication_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprism:publicationName\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprism:doi\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcited_by_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcitedby-count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maffiliation\u001b[39m\u001b[38;5;124m\"\u001b[39m: parse_affiliations(entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maffiliation\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])),\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcover_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprism:coverDate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mfetch_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscopus_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprism:volume\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen_access\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenaccess\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtypeDescription\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource-id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     })\n\u001b[0;32m     51\u001b[0m fetched_results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(entries)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(entries) \u001b[38;5;241m<\u001b[39m count:\n",
      "Cell \u001b[1;32mIn[5], line 71\u001b[0m, in \u001b[0;36mfetch_abstract\u001b[1;34m(scopus_id)\u001b[0m\n\u001b[0;32m     66\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mABSTRACT_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mscopus_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-ELS-APIKey\u001b[39m\u001b[38;5;124m\"\u001b[39m: API_KEY,\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m }\n\u001b[1;32m---> 71\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     74\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\http\\client.py:1423\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1423\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##api scopus\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "API_KEY = \"c31125018f2d22db977ac3d0cb28dd37\"\n",
    "BASE_URL = \"https://api.elsevier.com/content/search/scopus\"\n",
    "ABSTRACT_URL = \"https://api.elsevier.com/content/abstract/scopus_id/\"\n",
    "\n",
    "# Function to fetch Scopus data based on a query\n",
    "def fetch_scopus_data(query, count=25, max_results=100):\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": API_KEY\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"count\": count,\n",
    "        \"start\": 0,\n",
    "        \"view\": \"STANDARD\",\n",
    "        \"sort\": \"citedby-count\"\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "    fetched_results = 0\n",
    "\n",
    "    while fetched_results < max_results:\n",
    "        response = requests.get(BASE_URL, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            entries = data.get(\"search-results\", {}).get(\"entry\", [])\n",
    "            for entry in entries:\n",
    "                scopus_id = entry.get(\"dc:identifier\", \"\").replace(\"SCOPUS_ID:\", \"\")\n",
    "                all_results.append({\n",
    "                    \"url\": entry.get(\"prism:url\"),\n",
    "                    \"identifier\": scopus_id,\n",
    "                    \"eid\": entry.get(\"eid\"),\n",
    "                    \"title\": entry.get(\"dc:title\"),\n",
    "                    \"creator\": parse_authors(entry.get(\"dc:creator\")),\n",
    "                    \"publication_name\": entry.get(\"prism:publicationName\"),\n",
    "                    \"doi\": entry.get(\"prism:doi\"),\n",
    "                    \"cited_by_count\": int(entry.get(\"citedby-count\", 0)),\n",
    "                    \"affiliation\": parse_affiliations(entry.get(\"affiliation\", [])),\n",
    "                    \"cover_date\": entry.get(\"prism:coverDate\"),\n",
    "                    \"abstract\": fetch_abstract(scopus_id),\n",
    "                    \"volume\": entry.get(\"prism:volume\"),\n",
    "                    \"open_access\": entry.get(\"openaccess\"),\n",
    "                    \"subtype\": entry.get(\"subtypeDescription\"),\n",
    "                    \"source_id\": entry.get(\"source-id\")\n",
    "                })\n",
    "\n",
    "            fetched_results += len(entries)\n",
    "            if len(entries) < count:\n",
    "                break\n",
    "            params[\"start\"] += count\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: {response.status_code} - {response.reason}\")\n",
    "            break\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Function to fetch abstract using Scopus ID\n",
    "def fetch_abstract(scopus_id):\n",
    "    if not scopus_id:\n",
    "        return None\n",
    "\n",
    "    url = f\"{ABSTRACT_URL}{scopus_id}\"\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": API_KEY,\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"abstracts-retrieval-response\", {}).get(\"coredata\", {}).get(\"dc:description\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch abstract for {scopus_id}: {response.status_code} - {response.reason}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to parse authors\n",
    "def parse_authors(creator):\n",
    "    if isinstance(creator, dict) and \"author\" in creator:\n",
    "        authors = creator[\"author\"]\n",
    "        return \"; \".join([f'{a[\"ce:indexed-name\"]}' for a in authors])\n",
    "    return creator\n",
    "\n",
    "# Helper function to parse affiliations\n",
    "def parse_affiliations(affiliations):\n",
    "    return \"; \".join(\n",
    "        [f'{aff.get(\"affilname\", \"Unknown\")}, {aff.get(\"affiliation-city\", \"Unknown\")}, {aff.get(\"affiliation-country\", \"Unknown\")}' for aff in affiliations]\n",
    "    )\n",
    "\n",
    "# Fetch data using a query\n",
    "query = 'PUBYEAR > 2019 AND SUBJAREA(COMP) AND TITLE(\"computer\")'\n",
    "articles = fetch_scopus_data(query, count=10, max_results=100)\n",
    "\n",
    "# Save results to CSV\n",
    "output_filename = \"scopus_data_with_abstracts2.csv\"\n",
    "with open(output_filename, \"w\", newline='', encoding=\"utf-8\") as csv_file:\n",
    "    fieldnames = [\n",
    "        \"url\", \"identifier\", \"eid\", \"title\", \"creator\", \"publication_name\", \"doi\",\n",
    "        \"cited_by_count\", \"affiliation\", \"cover_date\", \"abstract\", \"volume\", \"open_access\", \n",
    "        \"subtype\", \"source_id\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for article in articles:\n",
    "        writer.writerow(article)\n",
    "\n",
    "print(f\"Fetched {len(articles)} articles with abstracts.\")\n",
    "print(f\"Saved to {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyOl0GUzxHG-"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved as 2020_all_file.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# กำหนดเส้นทางของโฟลเดอร์ที่เก็บไฟล์ JSON\n",
    "folder_path = \"Project/2020/\"  # แก้ไขเป็นเส้นทางของโฟลเดอร์ที่คุณต้องการ\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        # เปิดและโหลดไฟล์ JSON\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # เข้าถึงข้อมูลหลัก\n",
    "        abstracts_data = data.get(\"abstracts-retrieval-response\", {})\n",
    "\n",
    "        # ฟังก์ชันในการดึงข้อมูล\n",
    "        def extract_data(abstracts_data):\n",
    "            rows = []\n",
    "            # # ตรวจสอบว่า data มีข้อมูลใน \"affiliation\" หรือไม่\n",
    "            coredata = abstracts_data.get(\"coredata\", {})\n",
    "            subject_areas = abstracts_data.get(\"subject-areas\", {}).get(\"subject-area\", [])\n",
    "\n",
    "\n",
    "\n",
    "            affiliations = abstracts_data.get(\"affiliation\", [])\n",
    "            # Get the first affiliation, defaulting to an empty dictionary if there are no affiliations\n",
    "            first_affiliation = next(iter(affiliations), {})\n",
    "\n",
    "            # Ensure first_affiliation is a dictionary before accessing .get() method\n",
    "            first_affiliation_name = first_affiliation.get(\"affilname\", \"not have affiliation\") if isinstance(first_affiliation, dict) else \"not have affiliation\"\n",
    "            first_affiliation_city = first_affiliation.get(\"affiliation-city\", \"\") if isinstance(first_affiliation, dict) else \"not have affiliation\"\n",
    "            first_affiliation_country = first_affiliation.get(\"affiliation-country\", \"\") if isinstance(first_affiliation, dict) else \"not have affiliation\"\n",
    "            \n",
    "            def check_first_affiliation_name(a):\n",
    "                if a == \"not have affiliation\":\n",
    "                    return affiliations.get(\"affilname\", \"\")\n",
    "                else:\n",
    "                    return a\n",
    "            def check_first_affiliation_city(a):\n",
    "                if a == \"not have affiliation\":\n",
    "                    return affiliations.get(\"affiliation-city\", \"\")\n",
    "                else:\n",
    "                    return a\n",
    "            def check_first_affiliation_country(a):\n",
    "                if a == \"not have affiliation\":\n",
    "                    return  affiliations.get(\"affiliation-country\", \"\")\n",
    "                else:\n",
    "                    return a\n",
    "\n",
    "\n",
    "            # ประมวลผลข้อมูลจาก subject-areas\n",
    "            subjects_list = []\n",
    "            for area in subject_areas:\n",
    "                subject_details = {\n",
    "                    \"Name\": area.get(\"$\", \"\"),\n",
    "                    \"Abbreviation\": area.get(\"@abbrev\", \"\"),\n",
    "                }\n",
    "                subjects_list.append(subject_details)\n",
    "\n",
    "            # สร้างข้อมูลแถว\n",
    "            row = {\n",
    "                \"Title\": coredata.get(\"dc:title\", \"\"),\n",
    "                \"Abstract\": coredata.get(\"dc:description\", \"\"),\n",
    "                \"Publication Name\": coredata.get(\"prism:publicationName\", \"\"),\n",
    "                \"DOI\": coredata.get(\"prism:doi\", \"\"),\n",
    "                \"EID\": coredata.get(\"eid\", \"\"),\n",
    "                \"Cited By Count\": coredata.get(\"citedby-count\", \"\"),\n",
    "                \"Publication Date\": coredata.get(\"prism:coverDate\", \"\"),\n",
    "                \"Volume\": coredata.get(\"prism:volume\", \"\"),\n",
    "                \"ISSN\": coredata.get(\"prism:issn\", \"\"),\n",
    "                \"Article Number\": coredata.get(\"article-number\", \"\"),\n",
    "                \"Publisher\": coredata.get(\"dc:publisher\", \"\"),\n",
    "                \"Open Access\": coredata.get(\"openaccess\", \"\"),\n",
    "                \"Open Access Flag\": coredata.get(\"openaccessFlag\", \"\"),\n",
    "                \"Subtype Description\": coredata.get(\"subtypeDescription\", \"\"),\n",
    "                \"Source ID\": coredata.get(\"source-id\", \"\"),\n",
    "                \"First Affiliation Name\": check_first_affiliation_name(first_affiliation_name),\n",
    "                \"First Affiliation City\": check_first_affiliation_city(first_affiliation_city),\n",
    "                \"First Affiliation Country\": check_first_affiliation_country(first_affiliation_country),\n",
    "                \"Subject Areas\": subjects_list,\n",
    "                \"Scopus URL\": next((link[\"@href\"] for link in coredata.get(\"link\", []) if link[\"@rel\"] == \"scopus\"), \"\")\n",
    "            }\n",
    "\n",
    "            rows.append(row)\n",
    "            return rows\n",
    "\n",
    "        return extract_data(abstracts_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error in single file processing{file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# วนลูปเพื่อประมวลผลไฟล์ทั้งหมดในโฟลเดอร์\n",
    "all_rows = []\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Check if the path is a file (and not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                # Process the file and extend the results to all_rows\n",
    "                all_rows.extend(process_file(file_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "else:\n",
    "    print(f\"The folder path '{folder_path}' does not exist or is not a directory.\")\n",
    "\n",
    "\n",
    "\n",
    "# สร้าง DataFrame และบันทึกข้อมูลลงใน CSV เดียว\n",
    "df = pd.DataFrame(all_rows)\n",
    "output_csv = \"2020_all_file.csv\"  # ชื่อไฟล์ CSV ที่ต้องการ\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "print(f\"CSV file saved as {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved as 2023_all_file.csv\n"
     ]
    }
   ],
   "source": [
    "##this is for test folder \n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# กำหนดเส้นทางของโฟลเดอร์ที่เก็บไฟล์ JSON\n",
    "folder_path = \"Project/2023/\"  # แก้ไขเป็นเส้นทางของโฟลเดอร์ที่คุณต้องการ\n",
    "\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        # เปิดและโหลดไฟล์ JSON\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # เข้าถึงข้อมูลหลัก\n",
    "        abstracts_data = data.get(\"abstracts-retrieval-response\", {})\n",
    "\n",
    "        # ฟังก์ชันในการดึงข้อมูล\n",
    "        def extract_data(abstracts_data):\n",
    "            rows = []\n",
    "            # # ตรวจสอบว่า data มีข้อมูลใน \"affiliation\" หรือไม่\n",
    "            coredata = abstracts_data.get(\"coredata\", {})\n",
    "            subject_areas = abstracts_data.get(\"subject-areas\", {}).get(\"subject-area\", [])\n",
    "            authors_data = abstracts_data.get(\"authors\", {}).get(\"author\", [])\n",
    "\n",
    "\n",
    "            affiliations = abstracts_data.get(\"affiliation\", [])\n",
    "            # Get the first affiliation, defaulting to an empty dictionary if there are no affiliations\n",
    "            first_affiliation = next(iter(affiliations), {})\n",
    "\n",
    "            authors = [\n",
    "            f\"{author.get('ce:given-name', '')} {author.get('ce:surname', '')} ({author.get('ce:indexed-name', '')})\"\n",
    "            for author in authors_data\n",
    "            ]\n",
    "\n",
    "            # Ensure first_affiliation is a dictionary before accessing .get() method\n",
    "            first_affiliation_name = first_affiliation.get(\"affilname\", \"not have affiliation\") if isinstance(first_affiliation, dict) else \"not have affiliation\"\n",
    "            first_affiliation_city = first_affiliation.get(\"affiliation-city\", \"\") if isinstance(first_affiliation, dict) else \"not have affiliation\"\n",
    "            first_affiliation_country = first_affiliation.get(\"affiliation-country\", \"\") if isinstance(first_affiliation, dict) else \"not have affiliation\"\n",
    "            \n",
    "            def check_first_affiliation_name(a):\n",
    "                if a == \"not have affiliation\":\n",
    "                    return affiliations.get(\"affilname\", \"\")\n",
    "                else:\n",
    "                    return a\n",
    "            def check_first_affiliation_city(a):\n",
    "                if a == \"not have affiliation\":\n",
    "                    return affiliations.get(\"affiliation-city\", \"\")\n",
    "                else:\n",
    "                    return a\n",
    "            def check_first_affiliation_country(a):\n",
    "                if a == \"not have affiliation\":\n",
    "                    return  affiliations.get(\"affiliation-country\", \"\")\n",
    "                else:\n",
    "                    return a\n",
    "\n",
    "\n",
    "            # ประมวลผลข้อมูลจาก subject-areas\n",
    "            subjects_list = []\n",
    "            for area in subject_areas:\n",
    "                subject_details = {\n",
    "                    \"Name\": area.get(\"$\", \"\"),\n",
    "                    \"Abbreviation\": area.get(\"@abbrev\", \"\"),\n",
    "                }\n",
    "                subjects_list.append(subject_details)\n",
    "\n",
    "            # สร้างข้อมูลแถว\n",
    "            row = {\n",
    "                \"Title\": coredata.get(\"dc:title\", \"\"),\n",
    "                \"Abstract\": coredata.get(\"dc:description\", \"\"),\n",
    "                \"Publication Name\": coredata.get(\"prism:publicationName\", \"\"),\n",
    "                \"DOI\": coredata.get(\"prism:doi\", \"\"),\n",
    "                \"EID\": coredata.get(\"eid\", \"\"),\n",
    "                \"Author\": \"; \".join(authors),\n",
    "                \"Cited By Count\": coredata.get(\"citedby-count\", \"\"),\n",
    "                \"Publication Date\": coredata.get(\"prism:coverDate\", \"\"),\n",
    "                \"Volume\": coredata.get(\"prism:volume\", \"\"),\n",
    "                \"ISSN\": coredata.get(\"prism:issn\", \"\"),\n",
    "                \"Article Number\": coredata.get(\"article-number\", \"\"),\n",
    "                \"Publisher\": coredata.get(\"dc:publisher\", \"\"),\n",
    "                \"Open Access\": coredata.get(\"openaccess\", \"\"),\n",
    "                \"Open Access Flag\": coredata.get(\"openaccessFlag\", \"\"),\n",
    "                \"Subtype Description\": coredata.get(\"subtypeDescription\", \"\"),\n",
    "                \"Source ID\": coredata.get(\"source-id\", \"\"),\n",
    "                \"First Affiliation Name\": check_first_affiliation_name(first_affiliation_name),\n",
    "                \"First Affiliation City\": check_first_affiliation_city(first_affiliation_city),\n",
    "                \"First Affiliation Country\": check_first_affiliation_country(first_affiliation_country),\n",
    "                \"Subject Areas\": subjects_list,\n",
    "                \"Scopus URL\": next((link[\"@href\"] for link in coredata.get(\"link\", []) if link[\"@rel\"] == \"scopus\"), \"\")\n",
    "            }\n",
    "\n",
    "            rows.append(row)\n",
    "            return rows\n",
    "\n",
    "        return extract_data(abstracts_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"error in single file processing{file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# วนลูปเพื่อประมวลผลไฟล์ทั้งหมดในโฟลเดอร์\n",
    "all_rows = []\n",
    "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "    # Iterate over all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Check if the path is a file (and not a directory)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                # Process the file and extend the results to all_rows\n",
    "                all_rows.extend(process_file(file_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "else:\n",
    "    print(f\"The folder path '{folder_path}' does not exist or is not a directory.\")\n",
    "\n",
    "\n",
    "\n",
    "# สร้าง DataFrame และบันทึกข้อมูลลงใน CSV เดียว\n",
    "df = pd.DataFrame(all_rows)\n",
    "output_csv = \"2023_all_file.csv\"  # ชื่อไฟล์ CSV ที่ต้องการ\n",
    "df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "print(f\"CSV file saved as {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
